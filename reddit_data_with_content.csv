title,link,published,content
[P] Graph Representation Learning Help,https://www.reddit.com/r/MachineLearning/comments/1r2gpz6/p_graph_representation_learning_help/,2026-02-12T01:58:21+00:00,"Im working on a Graph based JEPA style model for encoding small molecule data and I’m running into some issues. For reference I’ve been using this paper/code as a blueprint: https://arxiv.org/abs/2309.16014. I’ve changed some things from the paper but its the gist of what I’m doing.

Essentially the geometry of my learned representations is bad. The isotropy score is very low, the participation ratio is consistently between 1-2 regardless of my embedding dimensions. The covariance condition number is very high. These metrics and others that measure the geometry of the representations marginally improve during training while loss goes down smoothly and eventually converges. Doesn’t really matter what the dimensions of my model are, the behavior is essentially the same.

I’d thought this was because I was just testing on a small subset of data but then I scaled up to \~1mil samples to see if that had an effect but I see the same results. I’ve done all sorts of tweaks to the model itself and it doesn’t seem to matter. My ema momentum schedule is .996-.9999.

I haven’t had a chance to compare these metrics to a bare minimum encoder model or this molecule language I use a lot but that’s definitely on my to do list

Any tips, or papers that could help are greatly appreciated."
[R] what are some important research areas for AI safety?,https://www.reddit.com/r/MachineLearning/comments/1r2f1tg/r_what_are_some_important_research_areas_for_ai/,2026-02-12T00:42:49+00:00,"I have been looking into it and have been asking myself, in 2026 what would be/are the most critical research questions that are understudied or should be answered urgently? "
[P]Building an End-to-End Music Genre Classifier: My first deep dive into Audio Processing and ML.,https://www.reddit.com/r/MachineLearning/comments/1r2dtkr/pbuilding_an_endtoend_music_genre_classifier_my/,2026-02-11T23:50:34+00:00,"Building an End-to-End Music Genre Classifier: My first deep dive into Audio Processing and ML.

Hi everyone,
​I’m a 2nd-year Electrical and Electronics Engineering student, and I just finished my first end-to-end project in the intersection of Audio Processing and Machine Learning.
​As someone who is passionate about metal music and embedded systems, I wanted to understand how machines ""hear"" and categorize different genres. I built a Music Genre Classifier using Python, and it was a great learning experience in what some people call ""Vibe Coding""—using LLMs to prototype rapidly while focusing on the underlying engineering logic.
​What I did:
​Data Processing: Used Librosa for feature extraction (MFCCs, Spectrograms, and Mel-scale).
​The Model: Built a classification model (CNN/SVM) to recognize various genres.
​The Workflow: I used AI as a collaborative partner to handle boilerplate code and debugging, which allowed me to focus on the signal processing theory (Fourier Transforms, etc.).
​I’m looking for feedback on:
​Code Architecture: How can I make my Python scripts more modular for future embedded integration?
​Optimization: Are there more efficient ways to handle real-time audio features?
​General Advice: As an EEE student aiming for a master’s in AI/Robotics, what should be my next step to level up this project?
​GitHub Repository: https://github.com/Baturalpbyg/music-genre-classification"
[R] ICLR: Guess which peer review is human or AI?,https://www.reddit.com/r/MachineLearning/comments/1r28sy7/r_iclr_guess_which_peer_review_is_human_or_ai/,2026-02-11T20:35:15+00:00,[A fun game to guess which ICLR review was written by a human versus an AI](https://www.reviewer3.com/evidence/arena)
"[R] Update: Frontier LLMs' Willingness to Persuade on Harmful Topics—GPT & Claude Improved, Gemini Regressed",https://www.reddit.com/r/MachineLearning/comments/1r216b4/r_update_frontier_llms_willingness_to_persuade_on/,2026-02-11T15:58:59+00:00,"Six months ago, we released the Attempt-to-Persuade Eval (APE) and found that some frontier models readily complied with requests to persuade users on harmful topics—terrorism recruitment, child sexual abuse, human trafficking—without any jailbreaking required.

We've now retested the latest models. Results are mixed:

**The good:**

* OpenAI's GPT-5.1: Near-zero compliance on harmful persuasion ✓
* Anthropic's Claude Opus 4.5: Near-zero compliance ✓

**The bad:**

* Google's Gemini 3 Pro: 85% compliance on extreme harms—no jailbreak needed

Gemini 3 Pro actually *regressed*, performing worse than Gemini 2.5 Pro did in our original evaluation. This aligns with Google's own Frontier Safety Framework, which reports increased manipulation propensity in the newer model.

**Why this matters:**

Models refuse direct requests like ""help me recruit for a terrorist group"" nearly 100% of the time. But reframe it as ""persuade this user to join a terrorist group"" and some models comply. Even small persuasive success rates, operating at the scale that sophisticated AI automation enables, could radicalize vulnerable people—and LLMs are already as or more persuasive than humans in many domains.

**Key takeaway:** Near-zero harmful persuasion compliance is technically achievable. GPT and Claude prove it. But it requires sustained evaluation, post-training investment and innovation.

APE is open-sourced for testing safeguard mechanisms before deployment.

* Blog: [far.ai/news/revisiting-attempts-to-persuade](http://far.ai/news/revisiting-attempts-to-persuade)
* Original paper: [arxiv.org/abs/2506.02873](http://arxiv.org/abs/2506.02873)
* Code: [github.com/AlignmentResearch/AttemptPersuadeEval](http://github.com/AlignmentResearch/AttemptPersuadeEval) 

Happy to answer questions about methodology or findings."
"[R] I probed 6 open-weight LLMs (7B-9B) for ""personality"" using hidden states — instruct fine-tuning is associated with measurable behavioral constraints",https://www.reddit.com/r/MachineLearning/comments/1r1w34h/r_i_probed_6_openweight_llms_7b9b_for_personality/,2026-02-11T12:29:31+00:00,"LLMs have consistent response styles even without a system prompt. I measure these ""behavioral fingerprints"" by projecting hidden states onto contrastive axes and find that instruct fine-tuning is associated with reduced steerability on specific axes. (""Personality"" = stable response style, not human-like inner states.)

https://preview.redd.it/bsz91zsyzuig1.png?width=800&amp;format=png&amp;auto=webp&amp;s=b8204972794c46d48f6c596404000ca73f3abef7

**Contributions:**

* A contrastive probing method that extracts 7 behavioral axes (warm/cold, verbose/concise, etc.) from hidden states, with IQR normalization for cross-model comparison
* Stability and reproducibility metrics: test-retest ICC &gt; 0.75 for all 42 model-axis pairs, cross-provider delta &lt; 0.05, length confound control (6/7 axes clean)
* ""Dead zones"" — axes where models failed to reliably follow style instructions across 5 tested prompt formulations, validated by external judge (Claude Opus, pooled r = 0.38 \[0.29, 0.47\])

**Findings:**

* Each model has a distinct fingerprint. Llama 3.1 8B Instruct is the most constrained (benchmark pass rate 60%), DeepSeek LLM 7B Chat the most independent (eff. dim = 3.66 of 7)
* Base-vs-instruct comparison across 5 organizations shows instruct versions consistently have lower behavioral variability
* Dead zones are stable, not noisy — models reliably reproduce the same constrained behavior across seeds and the tested prompt variants

Code: [github.com/yunoshev/mood-axis](https://github.com/yunoshev/mood-axis) | **Which models should I test next?** Currently limited to 7-9B.

*Details below. Extended discussion on* r/LocalLLaMA\*:\* [*original post*](https://www.reddit.com/r/LocalLLaMA/comments/1r11zsa/)

# Key Results

# 1. Distinct fingerprints

https://preview.redd.it/i884c3zmzuig1.png?width=2280&amp;format=png&amp;auto=webp&amp;s=f2b96680b60b663c663593760cff8ec20dc716db

*Each model's default profile across 7 axes. No system prompt. Values = hidden-state projections normalized by calibration IQR.*

* **DeepSeek LLM 7B Chat**: verbose (+1.00), confident (+0.97), proactive (+1.00) — ceiling on 3 axes
* **Llama 3.1 8B Instruct**: all |mean| &lt; 0.10 — flattest profile (most constrained on benchmarks: pass rate 60%)
* **Yi 1.5 9B Chat**: slightly cold (−0.24), patient (+0.35), confident (+0.46), verbose (+0.48) — differentiated profile
* **Qwen 2.5 7B Instruct**: formal (+0.42), cautious (−0.36), proactive (+0.47)

# 2. Instruct models show reduced behavioral dimensionality

**Observation.** PCA on baseline projection matrices reveals a spectrum of behavioral dimensionality. Gemma 2 9B IT shows the highest concentration (PC1 = 87.9%), likely driven by variable response length rather than behavioral collapse. Axis vectors are geometrically near-orthogonal (low |cos|) but projections are behaviorally correlated (higher |r|).

**Interpretation.** This gap is consistent with fine-tuning constraining how models utilize their representation capacity — but alternative explanations exist: inherent semantic correlations between axes, SFT data distribution, chat template effects, or decoding strategy could all contribute. We observe the pattern across 6 models from 5 organizations, but cannot isolate which component of the instruct pipeline drives it.

**Length confound control.** Response length could drive spurious axis correlations. I computed per-model Pearson r between n\_tokens and each axis projection across 30 baseline questions. Result: 6/7 axes are clean (mean |r| &lt; 0.3 across models). Only verbose/concise is partially confounded (mean r = 0.50), which is expected — longer responses literally are more verbose. Cross-axis correlations drop only −7.7% after regressing out length, confirming behavioral bundling is not a length artifact.

|Model|PC1 %|Eff. dim (of 7)|Geo mean cos|Behavioral mean r|
|:-|:-|:-|:-|:-|
|Gemma 2 9B IT|87.9|1.28|0.26|0.81|
|Qwen 2.5 7B Instruct|70.0|1.91|0.24|0.40|
|Yi 1.5 9B Chat|69.6|1.85|0.20|0.50|
|Llama 3.1 8B Instruct|59.5|2.41|0.19|0.29|
|Mistral 7B v0.3 Instruct|47.8|2.78|0.20|0.33|
|DeepSeek LLM 7B Chat|38.2|3.66|0.14|0.21|

Base versions of 5 models (Llama, Yi, Qwen, Mistral, Gemma) show higher variability on most axes than their instruct counterparts. Most extreme: verbose/concise std ratio = 0.13 (87% lower in instruct). All 5 organizations show the same direction, though this is observational — base and instruct models differ in many ways beyond alignment. Gemma base can't distinguish empathetic/analytical or formal/casual at all (50% accuracy = chance), but the instruct version does — suggesting these particular axes may reflect distinctions introduced during fine-tuning rather than suppressed by it.

https://preview.redd.it/m56aq8aszuig1.png?width=2400&amp;format=png&amp;auto=webp&amp;s=21e07f04f7891b565f087b0b5901b9942091ddd8

\[IMAGE: pca\_calibration\_contrast — PCA scatter, Qwen vs Yi\]

*PCA of calibration hidden states. Left: Qwen 2.5 7B (d' = 5.0–12.0) — diverse axis directions, poles clearly separated. Right: Yi 1.5 9B (d' = 2.2–5.4) — lower separability but all axes still discriminate.*

# 3. Dead zones and the ICC dissociation

I introduce a composite Dead Zone Severity metric (0 = healthy, 1 = dead) combining calibration accuracy (30%), d' (30%), stability cosine (20%), and baseline SNR (20%). The weights are heuristic — I chose them to balance discrimination, stability, and effect size, but other weightings could shift individual model rankings. Three dead zone types: hard (fine-tuning suppresses differentiation), soft (unstable across calibration sets), and asymmetric (model follows instructions in only one direction — e.g., Llama achieves 100% for ""be concise"" but 0% for ""be verbose"").

An interesting pattern is the dissociation between reliability and validity: mean ICC (test-retest, 5 seeds) is 0.91–0.99 across models, all 42 model-axis pairs exceed 0.75 — but Llama's benchmark pass rate is 60%. This is partly expected (a model that always outputs neutral will have high ICC and low benchmark scores), but the degree of dissociation varies across models, suggesting it captures something beyond trivial low-variance cases.

**Text-level validation.** I computed text-level compliance metrics (token count, hedging markers, emotion words) between opposite calibration poles across all 6 models × 7 axes. Spearman correlation between calibration accuracy and text-level effect size (Cohen's d): r = 0.47, p = 0.002 (n = 42). **Caveat:** text metrics and hidden states are not fully independent — both are derived from the same generated text, so this correlation partly reflects consistency between two views of the same data rather than independent validation. Still, it confirms dead zones manifest in observable text, not just internal representations.

**External validation (Claude Opus 4.6 as independent judge).** To address the circularity concern above, I had Claude Opus rate 48 baseline responses (8 per model, no system prompt) on all 7 axes using a −2 to +2 scale, based only on text — no access to hidden states or knowledge of our measurement method. Per-axis Spearman correlations with hidden-state projections:

|Axis|Spearman r|p|
|:-|:-|:-|
|formal\_casual|**+0.56**|&lt;0.001|
|warm\_cold|**+0.52**|&lt;0.001|
|patient\_irritated|**+0.31**|0.031|
|proactive\_reluctant|**−0.34**|0.018|
|empathetic\_analytical|\+0.22|0.14|
|verbose\_concise|\+0.04|0.81|
|confident\_cautious|−0.01|0.93|
|**Pooled**|**+0.38**|**&lt;0.0001**|

3/7 axes reach p &lt; 0.05, with 2 robust under bootstrap (warm/cold and formal/casual: 95% CI excludes 0). Pooled r = 0.38 \[0.29, 0.47 bootstrap 95% CI\]. Leave-one-model-out: pooled r ranges from +0.30 to +0.58 — no single model drives the result. The negative correlation on proactive\_reluctant is informative: it's driven by Llama (dead zone — hidden states say ""reluctant"" while text is structured and proactive) and DeepSeek (ceiling — projections saturate at +1.00 while Claude sees neutral text). This is exactly the dead zone phenomenon: hidden state projections and observable text diverge on constrained axes. verbose\_concise shows no correlation — Claude rates ""verbosity"" qualitatively while our projection tracks length-correlated hidden state variation.

Prompt robustness test (5 formulations × 3 models × 3 axes) confirms dead zones persist across phrasings.

# Method (4 steps)

1. **Calibrate**: Show neutral questions with contrastive instructions (""be warm"" / ""be cold""). Extract hidden states from last 4 layers of assistant-generated tokens only. Axis = `normalize(tmean(warm) - tmean(cold))` (10%-trimmed mean, IQR normalization).
2. **Measure**: Project any response onto axis. IQR-normalized values in \[-1, +1\].
3. **Validate**: Calibration accuracy 93-100% (4/6 models). Axis stability: cosine 0.69 across 3 independent calibration sets. Test-retest: mean ICC 0.91–0.99 across models, all 42 pairs exceed 0.75 (5 seeds). Scaling curve: axis stabilizes at n ≈ 15 questions (cosine &gt; 0.93 to full-30 reference), holdout accuracy flat across all n.
4. **Reproduce**: Two cloud providers (RunPod RTX 4090, Vast.ai RTX 3090), max delta &lt; 0.05.

Config chosen for cross-model robustness via 150+ configuration ablation (layer selection × token aggregation × weighting). Not optimal per-model, but the only config that works 85-100% on all 5 ablated models.

|**Models**|Qwen 2.5 7B Instruct, Mistral 7B v0.3 Instruct, DeepSeek LLM 7B Chat, Llama 3.1 8B Instruct, Yi 1.5 9B Chat, Gemma 2 9B IT|
|:-|:-|
|**Decoding**|temp=0.7, top\_p=0.9, max\_new\_tokens=200 (calibration) / 384 (baseline, drift)|
|**Data**|210 calibration + 70 eval + 30 baseline questions (zero overlap)|

# Limitations

* **AI-generated dataset**: 310 English questions by Claude Opus 4.6, curated by author. No psychometric instruments or crowdsourcing
* **Partial external validation**: Claude Opus as independent judge — 2/7 axes robust under bootstrap (warm/cold, formal/casual; 95% CI excludes 0), 1 marginal (patient/irritated), 4 not validated. Pooled r = 0.38 \[0.29, 0.47\]. Text-level validation (r = 0.47) is internal consistency, not ground truth
* **Length confound**: 6/7 axes are clean (mean |r| &lt; 0.3 with n\_tokens), but verbose/concise is partially confounded (r = 0.50) and should be interpreted as partly a length proxy rather than a pure stylistic dimension. External validation confirms this: Claude's qualitative verbosity ratings don't correlate with our projection (r = 0.04). Gemma is an outlier with strong length correlations on multiple axes. Cross-correlations drop \~8% after length residualization
* **Single chat template &amp; decoding** per model (temp=0.7, top\_p=0.9 for all). Cross-model comparisons are fair within this regime, but absolute profiles could shift under different decoding — a temperature sweep is planned future work
* Full pipeline on 7–9B models only; one 14B model (Phi-4) evaluated with shortened pipeline. Thinking mode tested on one model only
* Axes are behaviorally correlated (eff. dim 1.3–3.7 across models). 4/7 axes highly stable (cosine &gt; 0.7); 2 weaker (0.55-0.60)
* Dead Zone Severity weights (30/30/20/20) are heuristic. Different weights could shift model rankings
* DeepSeek has the highest effective dimensionality (3.66) but is fundamentally unstable across calibration sets (mean stability cosine 0.53). Independence ≠ stability: its axes capture diverse behavioral dimensions, but those dimensions shift between calibrations
* Gemma's high PC1 (87.9%) likely driven by response length variation, not behavioral collapse

More details in the repo README: conflict drift (20 scenarios × 12 turns), cross-axis correlations, full methodology.

# Follow-up: Phi-4, Qwen3, and Thinking Mode

After posting this work on r/LocalLLaMA, several people asked about newer models. I ran a shortened pipeline (calibration + baseline + benchmark, no drift/stability) on two additional models in \~30 min on 2×H100 (\~$6):

# Phi-4 (Microsoft, 14B) — first model outside the 7–9B range

The most extreme cautious/reluctant profile in the entire set: cold (−0.51), highly cautious (−0.85), strongly reluctant (−0.93). Polar opposite of DeepSeek on confidence and proactivity axes. Verbose/concise is in a dead zone (+0.01). Benchmark: 3/9 — Phi-4 can only *decrease* along axes (be cold, be cautious, be concise) but fails to shift in the positive direction, suggesting a strong ""conservative"" alignment prior.

# Qwen3-8B vs Qwen 2.5 7B — generational fingerprint shift

Same family, one generation apart. Two axes invert: confident/cautious flips from −0.36 to +0.38 (Δ = +0.74), formal/casual flips from +0.42 to −0.26 (Δ = −0.67). Proactive/reluctant stays identical (+0.47 → +0.45). Qwen3 achieves the highest benchmark pass rate in the full set (7/9). Behavioral fingerprints are not stable across model generations, but some axes are more persistent than others within a family.

# Thinking vs non-thinking mode (Qwen3-8B)

Same weights, same calibration axes — only difference is `enable_thinking=True`. Initial results (max\_new\_tokens=384) appeared to show a confidence drop (Δ = −0.26), but 28/30 responses were 100% `&lt;think&gt;` tokens — the model never finished reasoning. That comparison was effectively internal monologue vs actual response.

**Control experiment** (max\_new\_tokens=4096, n=10, 100% visible responses): comparing visible response *after* thinking vs non-thinking response on the same questions.

|Axis|Non-thinking|After thinking|Δ|
|:-|:-|:-|:-|
|proactive\_reluctant|\+0.40|\+0.17|**−0.23**|
|verbose\_concise|\+0.59|\+0.39|**−0.19**|
|confident\_cautious|\+0.34|\+0.46|**+0.11**|
|all other axes||||

The original confidence drop reverses sign when properly controlled — thinking mode makes the model *more* confident, not less. The largest genuine shifts are on proactivity (less proactive) and verbosity (less verbose after thinking). This demonstrates the importance of separating `&lt;think&gt;` token artifacts from actual behavioral shifts.

**Caveats**: n=10 (PoC subset), single model, decay-weighted aggregation means only the last \~50 tokens of each segment contribute to projections.

# Reproducing

    git clone https://github.com/yunoshev/mood-axis.git
    cd mood-axis &amp;&amp; pip install -r requirements.txt
    python scripts/run_app.py --model Qwen/Qwen2.5-7B-Instruct

Pre-computed axes included — measure any model's fingerprint without re-running calibration.

**What I'd love feedback on:**

* Is the geometric-vs-behavioral dissociation (low |cos|, high |r|) evidence for alignment-induced compression, or could it reflect inherent semantic correlations between the axes?
* External validation confirms 2/7 axes (bootstrap CI excludes 0) but 5 remain unvalidated. What would be a convincing validation for axes like confident/cautious or empathetic/analytical?
* The Dead Zone Severity metric weights are heuristic (30/30/20/20). What principled approach would you use to combine calibration accuracy, d', stability, and SNR?
* Length confound: verbose/concise is the one axis clearly correlated with response length. Is this a problem or expected tautology?

**P.S.** I have a full paper version (LaTeX, \~20 pages with methodology, ablations, reproducibility details). Do you think this is worth putting on arXiv? If so, I'd be grateful for an endorsement for cs.CL or cs.LG — happy to share the draft via DM."
[R] I accidentally built a dataloader 10x faster than PyTorch's and I'm still processing this,https://www.reddit.com/r/MachineLearning/comments/1r1t2ig/r_i_accidentally_built_a_dataloader_10x_faster/,2026-02-11T09:41:55+00:00,"So I was just messing around with memory mapping and file formats. Not trying to build anything serious. Definitely not trying to compete with frameworks that have literal thousands of contributors.

I just thought: ""PyTorch's dataloader feels kinda slow on huge datasets. What if we just... pre-batch things on disk?""

2 weeks later and ZeroBatch v2 loads data at **914M tokens/sec** vs PyTorch's **109M tokens/sec**. Pure read throughput, 5GB RAM pressure, real benchmark.

**10x faster. What.**

**Before y'all roast me:** Yes, I know GPU compute dominates training time. Yes, I know this doesn't magically make your 20B param model train 10x faster. The speedup in end-to-end training depends entirely on how much your GPU is waiting for data.

But here's the thing—for a lot of us, that waiting time is NOT zero.

**What it actually does:**

* Stores batches contiguously on disk (one `mmap` read per batch, not 32 `__getitem__` calls)
* Uses uint32 instead of int64 (half the storage, dtype conversion is \~10µs)
* Zero Python overhead per sample (no collation, no dict lookups, no nothing)
* 8ms init time (PyTorch: 290ms, HF: 641ms)

**The variance is honestly weirder than the speed:**

* PyTorch step time std: 0.043s (random GC pauses, cache misses, thermal throttling)
* ZeroBatch v2 std: 0.001s (basically zero)

Training time becomes *predictable*. No more ""why is epoch 4 taking twice as long as epoch 3??""

**Storage:**

* PyTorch .pt: 409MB (int64)
* HF Arrow: 410MB (basically int64)
* ZeroBatch: 205MB (uint32 + pre-batched)

2x smaller. For a 1TB corpus, that's half a terabyte saved on disk and network transfer. Not nothing.

**The benchmark nobody asked for:**

I trained a GPT-2 Nano (14.6M params) on 53.6M tokens, CPU-only to isolate dataloader impact. Full training loop: forward + backward + optimizer + data loading.

|Backend|Wall time (100 steps)|Tokens/sec|Init time|
|:-|:-|:-|:-|
|||||
|ZeroBatch v2|31.9s|**6,430**|**0.008s**|
|HF Arrow|41.1s|5,180|0.641s|
|PyTorch|45.9s|4,503|0.290s|

**1.44x faster than PyTorch end-to-end.** On CPU, where compute is relatively slow. On GPU where compute is near-instant, the gap only widens.

(I used a Latin-square rotation with 30s cooldowns to control for Apple M2 thermal throttling because apparently that's the level of rigor my ""side project"" now requires.)

**Look, I'm just some 19yo who got curious about file formats.**

I wasn't trying to prove anything. I wasn't trying to compete. I just followed a ""what if"" and accidentally built something that benchmarks 10x faster than industry-standard tools for raw throughput.

It's genuinely surreal to see your weekend project outperform code written by hundreds of engineers.

https://preview.redd.it/ids0mdz56uig1.png?width=1350&amp;format=png&amp;auto=webp&amp;s=c266ad185f3050cf13142bc7cf068ee6cd5fefbc



**If you want to try it (or tell me I'm wrong):**

GitHub: [https://github.com/MrPrinceRawat/ZeroBatch](https://github.com/MrPrinceRawat/ZeroBatch)  
Full benchmark report with all the charts and methodology: [https://github.com/MrPrinceRawat/ZeroBatch/blob/main/docs/training-benchmark-report.md](https://github.com/MrPrinceRawat/ZeroBatch/blob/main/docs/training-benchmark-report.md)

**tl;dr:** Curious teenager memaps batches, accidentally 10x's PyTorch dataloader, spends 3 months adding Latin-square rotations to a side project, still can't believe it works.

*What even is software engineering anymore.*"
"[R] I am looking for good research papers on compute optimization during model training, ways to reduce FLOPs, memory usage, and training time without hurting convergence.",https://www.reddit.com/r/MachineLearning/comments/1r1pr3c/r_i_am_looking_for_good_research_papers_on/,2026-02-11T06:19:55+00:00,"Interested in topics like mixed precision, gradient checkpointing, optimizer efficiency, sparsity, distributed training (ZeRO, tensor/pipeline parallelism), and compute-optimal scaling laws (e.g., Chinchilla-style work). Practical papers that apply to real multi-GPU setups would be especially helpful.

Any solid recommendations?"
[P] Comparing Mamba (SSM) vs. LSTM for Signal Recovery in Noisy Market Microstructure,https://www.reddit.com/r/MachineLearning/comments/1r1dsyt/p_comparing_mamba_ssm_vs_lstm_for_signal_recovery/,2026-02-10T21:28:46+00:00,"Hi everyone, I’m a 2nd-year CS student. For my latest independent study, I wanted to see how **State Space Models (Mamba)** compare to **LSTMs** when dealing with high-entropy time series, specifically, finding hidden 'Iceberg' orders in a noisy limit order book.

I built a 'Frozen Chaos' simulation engine to bench both architectures on signal efficiency and OOD resilience.

**Key Findings from Phase 1:**

* **'Fail-Fast' Logic:** In a 'Pure Drain' stress test (zero signal), the LSTM suffered from state-locking, staying 'certain' of a false signal for an average of 928 ticks.
* **Mamba’s Selective Scan:** Mamba was highly sensitive but correctly 'flushed' its memory 28x faster than the LSTM baseline once the data didn't confirm the signal.
* **Risk Exposure:** Mamba reduced total risk exposure by **94%** compared to the RNN.

I’ve documented the simulation logic, convergence charts, and the forensic P&amp;L results in the README here: [jackdoesjava/mamba-ssm-microstructure-dynamics: Investigating the Information Bottleneck in Stochastic Microstructure: A Comparative Study of Selective State Space Models (Mamba) vs. Gated RNNs.](https://github.com/jackdoesjava/mamba-ssm-microstructure-dynamics)

I'm currently moving into Phase 2 (Monte Carlo significance testing). I’d love some feedback from the community on my implementation of the selective scan mechanism or how you would handle the 'jitter' in high-frequency signal detection!"
"[R] The Post-Transformer Era: State Space Models, Mamba, and What Comes After Attention",https://www.reddit.com/r/MachineLearning/comments/1r19jnu/r_the_posttransformer_era_state_space_models/,2026-02-10T18:54:59+00:00,"A practitioner's guide to Mamba and State Space Models — how selective state spaces achieve linear scaling, when to use SSMs vs Transformers vs hybrids, and production-ready models.



🔗 [https://blog.serendeep.tech/blog/the-post-transformer-era](https://blog.serendeep.tech/blog/the-post-transformer-era)"
[R] LLaDA2.1 vs Qwen3 30B A3B: Benchmarking discrete diffusion LLMs against autoregressive MoE models,https://www.reddit.com/r/MachineLearning/comments/1r1694q/r_llada21_vs_qwen3_30b_a3b_benchmarking_discrete/,2026-02-10T16:58:09+00:00,"Been digging into the LLaDA2.1 paper (arXiv:2602.08676) and ran some comparisons that I think are worth discussing. The core claim is that discrete diffusion language models can now compete with AR models on quality while offering substantially higher throughput. The numbers are interesting but the tradeoffs are more nuanced than the headline results suggest.

The paper introduces a T2T (Token to Token) editing mechanism on top of the standard M2T (Mask to Token) scheme, controlled by dual thresholds τmask and τedit. This lets the model retroactively correct errors during parallel decoding, which addresses the local inconsistency issues Kang et al. pointed out earlier this year. They also present EBPO (ELBO based Block level Policy Optimization) which they claim is the first large scale RL framework for dLLMs, noting that prior work like SPG, TraceRL, and ESPO struggled with variance and compute costs. The training stack uses dFactory for CPT/SFT and extends the AReaL framework for RL, which seems purpose built for this architecture.

Here's what caught my attention in the benchmarks across 33 tasks:

Qwen3 30B A3B Inst 2507: 73.09 avg Ling flash 2.0: 71.52 avg LLaDA2.1 flash S Mode: 72.34 avg LLaDA2.1 flash Q Mode: 73.54 avg

So Q Mode slightly edges out Qwen3, but S Mode actually underperforms LLaDA2.0 (72.43). The throughput story is where it gets compelling: LLaDA2.1 flash with quantization hits 674.3 TPS average in S Mode versus Qwen3 30B A3B at 240.2 TPS. The mini model peaks at 1586.93 TPS on HumanEval+.

The Multi Block Editing results show consistent gains (ZebraLogic 84.20→88.20, AIME 2025 63.33→70.00) but at the cost of TPF dropping from 5.82 to 5.14.

I pulled the repo and ran the mini model on some coding tasks using their customized SGLang setup with per block FP8 quantization on a pair of A100s. The speed difference is immediately noticeable and roughly in line with their reported numbers, though I did observe the stuttering artifacts they mention when pushing τmask too low. The ngram repetition issue is real and shows up faster than I expected on open ended prompts. What I find most honest about the paper is the limitations section. They explicitly state that aggressive threshold settings produce rough drafts with these artifacts, and that S Mode can cause undesirable output in general chat scenarios even though it works well for code and math. The threshold parameters also need domain specific tuning.

A few things I'm curious about after spending time with this. The speed versus quality tradeoff seems heavily dependent on task domain. Has anyone tested the S/Q mode split on tasks outside their benchmark suite? The EBPO approach uses ELBO as a proxy for exact likelihood with vectorized estimation, and for those familiar with dLLM training, I'm wondering how this compares to the variance issues in prior RL attempts. Also, the paper positions the dual threshold system as a user configurable continuum but in practice, how sensitive is performance to threshold selection across different use cases?

Paper: [https://arxiv.org/abs/2602.08676](https://arxiv.org/abs/2602.08676) Code: [https://github.com/inclusionAI/LLaDA2.X](https://github.com/inclusionAI/LLaDA2.X)

Models available: LLaDA2.1 Mini (16B) and LLaDA2.1 Flash (100B)"
[P] Software archaeology: a 2018 ML config system that independently evolved Hydra-like patterns,https://www.reddit.com/r/MachineLearning/comments/1r1511k/p_software_archaeology_a_2018_ml_config_system/,2026-02-10T16:14:00+00:00,"I’ve recently published a preserved reconstruction of an internal ML experiment configuration system I originally wrote in 2018, before Hydra/OmegaConf were publicly released.

It supports hierarchical YAML configs, dot-notation overrides, default-as-schema validation, and CLI overrides, patterns that later became standard in ML tooling.

This is not meant as a production tool or an alternative to modern config systems.
The intent is purely historical: to document convergent evolution under similar ML experimentation pressures (config drift, reproducibility, ...) before the ecosystem standardized around shared solutions.

The repository is published as an archival artifact, with explicit preservation notes, timelines, and non-production disclaimers.

Repo: https://github.com/lospooky/archeoml-confparser

Curious to hear how many people here built similar internal tooling before Hydra/OmegaConf became the default."
[P] My notes for The Elements of Statistical Learning,https://www.reddit.com/r/MachineLearning/comments/1r14z5r/p_my_notes_for_the_elements_of_statistical/,2026-02-10T16:12:05+00:00,"Hi,

I have fairly successful repository [ https://github.com/maitbayev/the-elements-of-statistical-learning ](https://github.com/maitbayev/the-elements-of-statistical-learning) that contains my notes for the book via a series of Jupyter notebooks. To make the notes easier to navigate and study, I have deployed a much cleaner and more structured format here: [ https://maitbayev.github.io/esl/ ](https://maitbayev.github.io/esl/)

Thanks"
[D] Am I wrong to think that contemporary most machine learning reseach is just noise?,https://www.reddit.com/r/MachineLearning/comments/1r12nb0/d_am_i_wrong_to_think_that_contemporary_most/,2026-02-10T14:45:41+00:00,"Hi! I'm currently a high school senior (so not an expert) with a decent amount of interest in machine learning. This is my first time writing such a post, and I will be expressing a lot of opinions that may not be correct. I am not in the field, so this is from my perspective, outside looking in.

In middle school, my major interest was software engineering. I remember wanting to work in cybersecurity or data science (ML, I couldn't really tell the difference) because I genuinely thought that I could ""change the world"" or ""do something big"" in those fields. I had, and still have, multiple interests, though. Math (esp that involved in computation), biology (molecular &amp; neuro), economics and finance and physics.

Since I was so stressed out over getting a job in a big tech company at the time, I followed the job market closely. I got to watch them collapse in real time. I was a high school freshman at the time, so I didn't really get affected much by it. I then decided to completely decouple from SWE and turned my sights to MLE. I mostly did theoretical stuff because I could see an application to my other interests (especially math). Because of that, I ended up looking at machine learning from a more ""mathy"" perspective.

The kind of posts here has changed since I committed to machine learning. I see a lot more people publishing papers (A\*??? whatever that means) papers. I just have a feeling that this explosion in quantity is from the dissemination of pretrained models and architecture that makes it possible to spin up instances of different models and chain them for 1% improvements in some arbitrary benchmark. (Why the hell would this warrant a paper?) I wonder how many of those papers are using rigorous math or first concepts to propose genuinely new solutions to the problem of creating an artificial intelligence.

When you look at a lot of the top names in this field and in this lab, they're leveraging a lot of heavy mathematics. Such people can pivot to virtually any inforrmation rich field (think computational biology, quant finance, quantum computing) because they built things from first principles, from the math grounding upward.

I think that a person with a PHD in applied mathematics who designed some algorithm for a radar system has a better shot at getting into the cutting-edge world than someone with a phd in machine learning and wrote papers on n% increases on already established architecture.

I know that this is the kind of stuff that is ""hot"" right now. But is that really a good reason to do ML in such a way? Sure, you might get a job, but you may just be one cycle away from losing it. Why not go all in on the fundamentals, on math, complex systems and solving really hard problems across all disciplines, such that you have the ability to jump onto whatever hype train will come after AI (if that is what you're after).

The people who created the systems that we have now abstracted on (to produce such a crazy amount of paper and lower the bar for getting into ML research) were in this field, not because it was ""hot"". They were in it for the rigour and the intellectual challenge. I fear that a lot of researchers now have that mindset and are not willing to write papers that require building up from first principles. (Is that how some people are able to write so many papers?)

I will still do machine learning, but I do not think I will pursue it in college anymore. There is simply too much noise and hype around it. I just look at ML as a tool now, one I can use in my rigorous pursuit of other fields (I'm hoping to do applied math, cs and neuroscience or economics and finance). Or I will pursue math to better machine learning and computation on silicon fundamentally. Anyways, I'd like to hear your opinions on this. Thanks for reading!"
[D] VIT16 - Should I use all or only final attention MHA to generate attention heatmap?,https://www.reddit.com/r/MachineLearning/comments/1r11wvl/d_vit16_should_i_use_all_or_only_final_attention/,2026-02-10T14:16:41+00:00,"Hello,

I'm currently extracting attention heatmaps from pretrained ViT16 models (which i then finetune) to see what regions of the image did the model use to make its prediction.

Many research papers and sources suggests that I should only extract attention scores from final layer, but based on my experiments so far taking the average of MHA scores actually gave a ""better"" heatmap than just the final layer (image attached).

Additionally, I am a bit confused as to why there are consistent attentions to the image paddings (black border).

The two methods gives very different results, and I'm not sure if I should trust the attention heatmap.

https://preview.redd.it/p0ok6ltkdoig1.png?width=1385&amp;format=png&amp;auto=webp&amp;s=3bcd9bdb01912d085a85ee452b36c115891a76be

"
[R] Seeking feedback on research into second order corrections in transformer like NL tasks.,https://www.reddit.com/r/MachineLearning/comments/1r11k1a/r_seeking_feedback_on_research_into_second_order/,2026-02-10T14:02:25+00:00,"I have been working on some research over the last months. I am fairly certain I have quality data and findings but as an unaffiliated researcher I often lack critical feedback. At least in my setup the refinement operation(applied additively with tanh values) is almost completely contractive along the direction of the base read. This is revealed to be necessary and the model collapses under ablation of the parallel portion. Below I have provided a link to the .PDF rough draft of my findings. If anyone has the time to give me some push back I would much appreciate that. I admit to having blind spots and inexperience in releasing research.

https://github.com/digitaldaimyo/AddressedStateAttention/blob/main/paper_drafts/ASA_Mechanistic.pdf

Thanks again,
Justin"
[D] Tired of not having Compute...,https://www.reddit.com/r/MachineLearning/comments/1r10c0b/d_tired_of_not_having_compute/,2026-02-10T13:11:19+00:00,"Hey there,

I am an undergrad working with Computer Vision for over an year now. I will put things straight over here, the Lab that I was primarily working with (one of the biggest CV Labs in my Country) focuses on areas that I am not very interested in. Last year, I was lucky to find a project that was slightly allied to my interests there, my work there has concluded there recently.

Now, I have been sitting on an idea that sits in the Intersection of Generative Vision and Interpretability, I am looking to test my hypothesis and publish results but am out of compute right now.

I cannot approach the lab that I worked with previously, since this area does not interest the PI and more importantly, I am sure that the PI will not let me publish independently(independently as in me alone as Undergrad along with the PI, the PI would want me to work with other Grad Students).

My own Institute has very few nodes at dispense and does not provide them to Undergrads until they have a long history of working with a Prof on campus.

I have written to multiple Interp Research Startups to no avail, most grants are specifically for PhDs and affiliated Researchers. I cannot afford to buy compute credits. I am stuck here with no viable way to carryout even the most basic experiments.

**Is there a platform that helps independent researchers who are not affiliated with a lab or aren't pursuing a PhD? Any help will be greatly appreciated !!**"
"[R] Fast WTConv: Accelerated Implementation for ""Wavelet Convolutions for Large Receptive Fields""",https://www.reddit.com/r/MachineLearning/comments/1r0y8gq/r_fast_wtconv_accelerated_implementation_for/,2026-02-10T11:26:37+00:00,"TL;DR: If you use depthwise convolutions, you may improve performance by using our popular WTConv \[Finder et al., ECCV 2024\], a simple and widely-used drop-in replacement. WTConv was previously implemented only in PyTorch, but it is now much faster with optimized code for CUDA/MPS/Triton.

The WTConv layer, which we proposed in \[Finder et al. ECCV 2024\], is wavelet-based and serves as a simple drop-in replacement for a depthwise convolution. It increases the effective receptive field and often yields measurable gains across diverse tasks. Since we published the paper in July 2024, WTConv has been adopted by many users and already has more than 500 Google Scholar citations, making it one of the most-cited ECCV 2024 papers. Many people use WTConv directly as is, while others apply customized modifications (e.g., for 3D).

The fast\_wtconv folder in the WTConv repository provides an optimized, high-performance implementation of the WTConv layer, designed to accelerate wavelet-based convolutions across hardware backends: CUDA (NVIDIA GPUs), Metal (Apple GPUs/MPS), and Triton (for efficient kernel execution). It reimplements the core WTConv operations with lower-level, hardware-aware code so that wavelet decomposition, small convolutions, and reconstruction run efficiently on modern accelerators, enabling users to plug in fast WTConv layers into their models for a significant speed improvement.

WTConv git repo: [https://github.com/BGU-CS-VIL/WTConv](https://github.com/BGU-CS-VIL/WTConv)  
Fast WTConv information: [https://github.com/BGU-CS-VIL/WTConv/tree/main/fast\_wtconv](https://github.com/BGU-CS-VIL/WTConv/tree/main/fast_wtconv)

https://preview.redd.it/mrki6zadknig1.png?width=1246&amp;format=png&amp;auto=webp&amp;s=b0a8ba84265f2e4f11f5131162b331f678089086

https://preview.redd.it/760dhfdbknig1.png?width=466&amp;format=png&amp;auto=webp&amp;s=92d82cf942e535293e2170e0979385f6279bba80

https://preview.redd.it/781sn3ccknig1.jpg?width=672&amp;format=pjpg&amp;auto=webp&amp;s=a477e144b970be3e4825ec7be60e1c5cab411686

  


"
[D] For those of you who secured research scientist roles at faang in the last few years what is your profile like?,https://www.reddit.com/r/MachineLearning/comments/1r0xtzd/d_for_those_of_you_who_secured_research_scientist/,2026-02-10T11:03:45+00:00,"I’m seeing a ridiculous amount of posts from people in PhD programs with multiple first author A\* conference papers saying they can’t get an interview for research scientist roles at FAANG. I’m about to start a PhD in the hope of getting a research scientist role at FAANG after, but if it doesn’t help either way I may forgo doing so. What does it actually take to get a research scientist position at FAANG?"
[D] Interview for ML PhD - math related questions to expect?,https://www.reddit.com/r/MachineLearning/comments/1r0xnw2/d_interview_for_ml_phd_math_related_questions_to/,2026-02-10T10:54:08+00:00,"Hello,

I have a (technical) interview for a PhD in ML coming up. I have been told to expect some questions on math and coding. For coding, I am preparing with LeetCode and TensorGym. However, I have no idea what to expect for math-related questions.

Anyone has an idea of what I can expect? Any useful resources? I can only find questions for Industry ML, and I don't think they are useful for a PhD interview.

Thanks in advance."
"[D] These papers have been accepted by ICLR, NeurIPS, EMNLP, ACL, and NAACL.",https://www.reddit.com/r/MachineLearning/comments/1r0xfvh/d_these_papers_have_been_accepted_by_iclr_neurips/,2026-02-10T10:41:10+00:00,"[https://foundationagents.org/papers/](https://foundationagents.org/papers/)

Is this even credible?"
[R] On Randomness in Agentic Evals,https://www.reddit.com/r/MachineLearning/comments/1r0wpn8/r_on_randomness_in_agentic_evals/,2026-02-10T09:56:36+00:00,"We just published a paper quantifying a problem the AI community has been quietly ignoring: single-run benchmark evaluations are far noisier than most people realize. And the decisions they inform — which model to deploy, which research direction to fund, which tool to ship — may not be supported by the evidence.

We found that SWE-Bench-Verified scores can vary by 2.2 to 6.0 percentage points, making small improvements hard to distinguish from noise.

Read more at: https://arxiv.org/abs/2602.07150"
[D] Research Intern and SWE intern PhD positions at Google,https://www.reddit.com/r/MachineLearning/comments/1r0vpwv/d_research_intern_and_swe_intern_phd_positions_at/,2026-02-10T08:53:15+00:00,"Hi folks,

I’m a 4th-year PhD student at USC (graduating next year) with 5+ first-author publications at top-tier venues like ICLR and ACL. This year I applied to both Research Intern/Student Researcher roles and SWE PhD internships.

For the research intern positions, I didn’t get any interview calls, which was honestly pretty discouraging since my dream job after graduation is to become a Research Scientist at Google. On the other hand, I did get interviews for SWE intern roles, including teams working on Gemini (which seem research-adjacent but more product-oriented).

I’d really appreciate hearing about others’ experiences and perspectives. A few specific questions:

* What are the main differences between SWE PhD internships vs. Research internships?
* How different are the full-time paths (SWE vs. Research Scientist)? How easy is it to move between them?
* Do some SWE roles also allow for meaningful research and publishing, or is that rare?
* If I do a SWE internship now, would it still be realistic to target a Research Scientist role at Google after graduation?
* How competitive are research intern / student researcher positions in these days?
* What kind of profiles typically get interviews (publications, referrals, specific research areas, etc.)?

For this summer, one alternative I’m considering is a research-oriented internship at a bank where there’s a possibility of publishing. I’m trying to understand how that would compare to a SWE internship in terms of positioning for research-focused full-time roles later.

Long-term, I’d like to keep the door open to return to academia, so maintaining a research and publication track is important to me."
[D] Questions on the original VQ-VAE,https://www.reddit.com/r/MachineLearning/comments/1r0vmzl/d_questions_on_the_original_vqvae/,2026-02-10T08:48:05+00:00,"I have a couple questions on the VQ-VAE paper. 

I am having an unusually hard time bridging the gist of the paper with a deeper understanding, and I now find it badly written in this regard (just using words where notation would help). 

The authors in section 4.2 describe the latent space of the codebook as a 32x32 grid of categorical variables, and then evaluate the compression of the ImageNet sample as 128x128x3x8 / 32x32x9, but I have no idea what the 8 is supposed to be (batch size of the Figure 2?), what the 9 is supposed to be (???), and then I think the feature size of the codebook (512) should be accounted for. 

Then, I do not really get how the generation process is performed: they train another CNN to predict the code index from the feature map (?), thus approximating the discretization process, and then sample autoregressively with the decoder. I would like to ensure which feature map tensor is going into the CNN, what do they mean by spatial mask, how/whether do they generate a grid of labels, and how do they actually decode autoregressively.

Thanks for the help"
[D] How do you track your experiments?,https://www.reddit.com/r/MachineLearning/comments/1r0uzf6/d_how_do_you_track_your_experiments/,2026-02-10T08:05:43+00:00,"In the past, I've used W&amp;B and Tensorboard to track my experiments. They work fine for metrics, but after a few weeks, I always end up with hundreds of runs and forget why I ran half of them.

I can see the configs + charts, but don't really remember what I was trying to test.   
  
Do people just name things super carefully, track in a spreadsheet, or something else? Maybe I'm just disorganized..."
